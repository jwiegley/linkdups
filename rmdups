#!/usr/bin/env python
#
# linkdups, version 1.2, by John Wiegley <johnw@gnu.org>
#
# usage: linkdups [--verbose] <DIRS...>

import os
import sys
import hashlib
import string
import filecmp
import optparse

from os.path import *
from stat import *

files        = {}
minimum_size = -1
maximum_size = -1

parser = optparse.OptionParser()
parser.add_option("-v", "--verbose",
                  action="store_true", dest="verbose", default=False,
                  help="report activity verbosely")
parser.add_option("-n", "--dry-run",
                  action="store_true", dest="dry_run", default=False,
                  help="make no changes to disk")

(options, args) = parser.parse_args()

def record_size_match(path):
    size = os.lstat(path)[ST_SIZE]

    if size == 0:
        return
    if minimum_size > 0 and size < minimum_size:
        return
    if maximum_size > 0 and size >= maximum_size:
        return

    if not files.has_key(size):
        files[size] = [path]
    else:
        files[size].append(path)

def record_checksum_match(path, files):
    fd = open(path)
    csum = hashlib.md5()
    for line in fd:
        csum.update(line)
    fd.close()

    digest = csum.digest()
    if not files.has_key(digest):
        files[digest] = [path]
    else:
        files[digest].append(path)

counter = 0
def find_matches(path, depth):
    global counter
    entries = os.listdir(path)
    if depth <= 1 and options.verbose:
        print ".. Scanning for entries in %s" % path
    for entry in entries:
        if entry.endswith(".dtBase2") or entry.endswith(".sparsebundle") :
            continue

        counter += 1
        if counter % 10000 == 0 and options.verbose:
            print ".. Scanned %d entries" % counter

        entry = join(path, entry)

        mode = os.lstat(entry)[ST_MODE]
        if S_ISDIR(mode) and \
           entry not in ("/proc" "/dev" "/sys" "/mnt"):
            find_matches(entry, depth + 1)
        elif S_ISREG(mode):
            record_size_match(entry)

bytes_saved = 0

def link_duplicates():
    global bytes_saved

    keys = files.keys()
    keys.sort()
    keys.reverse()

    for key in keys:
        value = files[key]
        if len(value) < 2:
            continue

        if options.verbose:
            print ".. Scanning %d files of size %d for checksum matches" % \
                (len(value), key)

        subfiles = {}
        for file in value:
            record_checksum_match(file, subfiles)

        size = key
        for key, value in subfiles.items():
            if len(value) < 2:
                continue

            if options.verbose:
                print " --> Found a group of %d files of size %d, based on MD5" % \
                    (len(value), size)

            fileInfo  = {}
            referent  = None
            unmatched = 0

            for file in value:
                info = os.lstat(file)
                if info[ST_NLINK] > 1:
                    referent = file
                else:
                    unmatched += 1

            if not unmatched:
                continue

            if options.verbose:
                print "   ==> of these, %d of the matching files are unpaired" % unmatched

            if not referent:
                referent = value[0]

            first = True
            for file in value:
                if file is referent:
                    continue

                # Only relink the file if it's not already linked.  This
                # misses matches between disparate groups, however, such as
                # when Time Machine re-dup's a file after you change its
                # permissions.
                #info = statInfo(file)
                #if info[ST_NLINK] > 1:
                #    continue

                # Before really deleting the file, verify that it's byte-wise
                # identical.  MD5 collisions can occur, albeit rarely.
                if not filecmp.cmp(file, referent):
                    continue

                try:
                    if options.dry_run:
                        print "os.remove(\"%s\")" % file
                    else:
                        os.remove(file)
                finally:
                    # In case the users presses Control-C right between removing
                    # and linking
                    if options.dry_run:
                        print "os.link(%s, %s)" % (referent, file)
                    elif not os.path.exists(file):
                        os.link(referent, file)

                bytes_saved += info[ST_SIZE]
                if options.verbose:
                    if first:
                        print "    %s ->" % referent
                        first = False
                    print "       %s" % file

def bytestring(amount):
    if amount < 1024:
        return "%d bytes" % amount
    elif amount < 1024 * 1024:
        return "%d KiB" % (amount / 1024)
    elif amount < 1024 * 1024 * 1024:
        return "%.1f MiB" % (amount / (1024.0 * 1024.0))
    elif amount < 1024 * 1024 * 1024 * 1024:
        return "%.2f GiB" % (amount / (1024.0 * 1024.0 * 1024.0))

if len(args) > 0:
    paths = args
else:
    paths = ["."]

for path in paths:
    if isfile(path):
        record_size_match(entry)
    elif not isdir(path):
        print "%s is not a directory!" % path
    else:
        if options.verbose:
            print ".. Scanning for large files in %s" % path
        maximum_size = -1
        minimum_size = 16 * 1024
        find_matches(path, 0)
        if options.verbose:
            print ":: Scanned %d files for size matches" % len(files.items())

        if options.verbose:
            print ".. Scanning for small files in %s" % path
        maximum_size = 16 * 1024
        minimum_size = -1
        find_matches(path, 0)
        if options.verbose:
            print ":: Scanned %d files for size matches" % len(files.items())

try:
    link_duplicates()
finally:
    print "Saved %s" % bytestring(bytes_saved)
